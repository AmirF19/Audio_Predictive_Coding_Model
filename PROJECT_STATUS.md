# Project Status: Predictive Coding Model of Audio Input & N400

**Last Updated**: December 1st, 2025  
**Authors**: Alba Jorquera, Muhammad Fusenig, William Zumchak

---

## Current Project

A hierarchical predictive model that simulates the processesing of spoken words. Specifically, it models the **N400** and surprisal.

### Premise

We compare neural data from a semantic priming task and a predictive coding model to see if the model predicts the same pattern of effects across noisy and clear audio conditions.

---

## Current Status: Working Model

1. Takes audio input (Wav2Vec embeddings of spoken words) decomposed into phonemes
2. Processes it through a 3-layer predictive coding network
3. Generates simulated N400 responses
4. Matches the expected pattern from human experiments

---

## The Two Model Versions

### 1. Llama Semantics (`Predictive_Coding_Model/`)

- Uses semantic features generated by Llama 3.1
- 13,009 binary features (e.g., "is_animal", "has_fur")
- Sparse, interpretable representations

### 2. Word2Vec (`Predictive_Coding_Model_Word2Vec/`)

- Uses pre-trained Word2Vec embeddings
- 300 continuous dimensions
- Dense, distributional representations

**Both models show the same qualitative pattern**

---

## How to Run It

```bash
# Activate the virtual environment
cd comp_ling_project
venv\Scripts\activate

# Run Llama version
cd Predictive_Coding_Model
python simulation.py

# Run Word2Vec version
cd ..\Predictive_Coding_Model_Word2Vec
python simulation.py
```

Each run takes about 13-15 minutes on the RTX 3090.

---

## Key Results

### Clear Speech (Priming Works)
```
Same word:    N400 = 0.13  (lowest - repetition benefit)
Similar word: N400 = 0.33  (high - partial mismatch)
Different:    N400 = 0.30  (high - full mismatch)
```

### Noisy Speech (Priming Disrupted)
```
Same word:    N400 = 0.18  (all similar - can't recognize word)
Similar word: N400 = 0.20
Different:    N400 = 0.20
```

The pattern flattens because the model can't recognize the noisy word, so it can't benefit from priming.

---

## Connection to Will's Experiment

This model was designed to simulate findings from Zumchak et al.'s (2025) auditory priming study:

- **2x2 design**: Semantic similarity × Auditory clarity
- **Key finding**: In distorted speech, N400 differences disappear, but behavioral ratings may still show priming

The model captures the N400 pattern. Next step could be adding an "intelligibility" metric to capture the behavioral side.

---

## Files

| File | Purpose |
|------|---------|
| `simulation.py` | Main experiment script - run this |
| `pc_model.py` | The predictive coding model class |
| `data_loader.py` | Loads audio and semantic data |
| `config.py` | All parameters (change settings here) |
| `results/` | Output CSV and plots go here |

---

## Data Pipeline

```
Audio recordings → Wav2Vec 2.0 → Phoneme vectors (.npy files)
                                        ↓
                               PC Model (simulation.py)
                                        ↓
                               N400 predictions
```

The phoneme vectors are pre-computed and stored in `audio_phonemes/PC_Input_Vectors/`.

---

## Potential extensions:

1. **Add sentential context**: Instead of single-word primes, use sentence context
2. **Compare to experimental data**: Validate against Zumchak et al.'s (2025) results

---

## Technical Notes

- Tested on NVIDIA RTX 3090
- ~ 800 words in the lexicon, 1600+ audio files (clear + noisy)

