# Project Status: Predictive Coding Model of Audio Input & N400

**Last Updated**: December 1st, 2025  
**Authors**: Alba Jorquera, Muhammad Fusenig, William Zumchak

---

## What This Project Does

This is a computational model that simulates how the brain processes spoken words. Specifically, it models the **N400** - a brain response that happens about 400ms after hearing a word, which reflects how surprising or expected that word is.

### The Core Idea

When you hear a word, your brain is constantly predicting what comes next. If you hear "dog" and then "cat", there's a small surprise (they're related but different). If you hear "dog" and then "table", there's a bigger surprise (unrelated). This surprise shows up as the N400 in EEG/MEG experiments.

---

## Current Status: Working Model

We have a **fully functional model** that:

1. Takes audio input (Wav2Vec embeddings of spoken words)
2. Processes it through a 3-layer predictive coding network
3. Generates simulated N400 responses
4. Matches the expected pattern from human experiments

### What We've Validated

| Finding | Status |
|---------|--------|
| Repetition priming reduces N400 | ✓ Working |
| Semantic mismatch increases N400 | ✓ Working |
| Noise disrupts the N400 pattern | ✓ Working |

---

## The Two Model Versions

### 1. Llama Semantics (`Predictive_Coding_Model/`)

- Uses semantic features generated by Llama 3.1
- 13,009 binary features (e.g., "is_animal", "has_fur")
- Sparse, interpretable representations

### 2. Word2Vec (`Predictive_Coding_Model_Word2Vec/`)

- Uses pre-trained Word2Vec embeddings
- 300 continuous dimensions
- Dense, distributional representations

**Both models show the same qualitative pattern**, which suggests the core finding is robust.

---

## How to Run It

```bash
# Activate the virtual environment
cd comp_ling_project
venv\Scripts\activate

# Run Llama version
cd Predictive_Coding_Model
python simulation.py

# Run Word2Vec version
cd ..\Predictive_Coding_Model_Word2Vec
python simulation.py
```

Each run takes about 13-15 minutes on the RTX 3090.

---

## Key Results

### Clear Speech (Priming Works)
```
Same word:    N400 = 0.13  (lowest - repetition benefit)
Similar word: N400 = 0.33  (high - partial mismatch)
Different:    N400 = 0.30  (high - full mismatch)
```

### Noisy Speech (Priming Disrupted)
```
Same word:    N400 = 0.18  (all similar - can't recognize word)
Similar word: N400 = 0.20
Different:    N400 = 0.20
```

The pattern flattens because the model can't recognize the noisy word, so it can't benefit from priming.

---

## Connection to Will's Experiment

This model was designed to simulate findings from Will's auditory priming study:

- **2x2 design**: Semantic similarity × Auditory clarity
- **Key finding**: In distorted speech, N400 differences disappear, but behavioral ratings may still show priming

The model captures the N400 pattern. Next step could be adding an "intelligibility" metric to capture the behavioral side.

---

## Files You Need to Know

| File | Purpose |
|------|---------|
| `simulation.py` | Main experiment script - run this |
| `pc_model.py` | The predictive coding model class |
| `data_loader.py` | Loads audio and semantic data |
| `config.py` | All parameters (change settings here) |
| `results/` | Output CSV and plots go here |

---

## Data Pipeline

```
Audio recordings → Wav2Vec 2.0 → Phoneme vectors (.npy files)
                                        ↓
                               PC Model (simulation.py)
                                        ↓
                               N400 predictions
```

The phoneme vectors are pre-computed and stored in `audio_phonemes/PC_Input_Vectors/`.

---

## What's Next?

Potential extensions:

1. **Add sentential context**: Instead of single-word primes, use sentence context
2. **Model intelligibility ratings**: Add a behavioral metric alongside N400
3. **Parameter fitting**: Fit model parameters to match real EEG data
4. **Compare to experimental data**: Validate against Will's actual results

---

## Questions?

The code is documented and should be readable. Key concepts:

- **Prediction error**: Difference between expected and actual activation
- **Precision**: How much weight to give the input signal
- **Semantic expectation**: What the model "expects" after the prime

Feel free to reach out if anything is unclear.

---

## Technical Notes

- Runs on GPU (CUDA) if available, falls back to CPU
- Tested on NVIDIA RTX 3090
- Python 3.10, PyTorch 2.x
- About 800 words in the lexicon, 1600+ audio files (clear + noisy)

